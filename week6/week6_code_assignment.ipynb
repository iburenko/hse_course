{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78h0D99QgQ8E"
   },
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAuHZ90xgUOM"
   },
   "source": [
    "- The maximum number of points for this assignment is 10, the minimum number of points is 0.\n",
    "- You have two weeks to complete the assignment. Once the assignment is submitted you are not allowed to change it.\n",
    "- One week delay is penalized with 1 point.\n",
    "  - Example. The assignment is issued on the 1st January. The deadline without penalization is until 23:59 January 14th (anywhere on Earth). Student A submits his assignment on 22:51 January 13th and is not penalized; student B submits his assignment on 01:13 January 15th and is penalized with 1 point; student C submits his assignment on 3:56 January 23th and is penalized with 2 points and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19BBYvTLo6eI"
   },
   "source": [
    "# Week 6 assignment\n",
    "\n",
    "- **TASK**\n",
    "- - In this week we will classify slices from CT scans into two classes: healthy and affected by covid. \n",
    "- - We will learn how to project embeddings from the high-dimensional latent space into the plane;\n",
    "- - We will implement _Expected Calibration Error_ in order to measure how well our model is calibrated;\n",
    "- - Finally we will perform temperature scaling in order to reduce the _ECE_\n",
    "\n",
    "- **DATA**\n",
    "- - We will make use of segmentation data from the week 3;\n",
    "- - If a slice has less or equal 20 pixels corresponding to covid we will treat this sample as a member of the negative class; if there are more than 20 pixels on the slide then we consider this sample as a member of the positive class;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WuewW1Kgf3T"
   },
   "source": [
    "# Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCWDyrBpJLuj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "# Downloading files\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "!wget -nc https://github.com/iburenko/hse_course/raw/main/week3/data/corona_train_segm_dataset.npz\n",
    "!wget -nc https://github.com/iburenko/hse_course/raw/main/week3/data/corona_val_segm_dataset.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9-YMPS4JbL3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "# Install required packages into the environment\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "!pip -q install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yywRenuBR-BF"
   },
   "outputs": [],
   "source": [
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "# Import needed packages into the environment\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms as t\n",
    "from timm import list_models, create_model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTQz0cuLjVYV"
   },
   "source": [
    "# Load data into the environment\n",
    "\n",
    "- **Normalisation**:\n",
    "In this notebook we will use a different normalisation compared to those that we used in week 3. We will clip outlying values, namely lower than 0.01 and higher than 0.99 percentiles and then we will use 0-1 normalisation, meaning that we subtract from the dataset its minimum value and divide the result of subtraction to the maximum value of the dataset;\n",
    "- Feel free to change the normalisation if you find it appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eiZZPDcpJZ0o"
   },
   "outputs": [],
   "source": [
    "train_data = dict(np.load('../week3/data/corona_train_segm_dataset.npz', allow_pickle=True))\n",
    "val_data = dict(np.load('../week3/data/corona_val_segm_dataset.npz', allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bh_MelMmLkoN"
   },
   "outputs": [],
   "source": [
    "lower_bound = np.quantile(train_data['images'], 0.01)\n",
    "upper_bound = np.quantile(train_data['images'], 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDSxt0BTkLXj"
   },
   "source": [
    "# Define Dataset class\n",
    "\n",
    "- In this variant we use some random augmentations to transform the training data;\n",
    "- No augmentation is used for the validation dataset;\n",
    "- Feel free to change the set of augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Re3LtFWHIY10"
   },
   "outputs": [],
   "source": [
    "class SegmentationDatasetNumpy(Dataset):\n",
    "    def __init__(self, train, lower_bound, upper_bound):\n",
    "        super().__init__()\n",
    "        assert train in ['train', 'val']\n",
    "        self.train = train\n",
    "        if self.train == 'train':\n",
    "            data = train_data\n",
    "            data_transform = t.Compose(\n",
    "              [\n",
    "                  t.RandomRotation(15),\n",
    "                  t.RandomHorizontalFlip(),\n",
    "                  t.RandomVerticalFlip(),\n",
    "                  t.GaussianBlur(3),\n",
    "              ]\n",
    "          )\n",
    "        else:\n",
    "            data = val_data\n",
    "            data_transform = t.Compose([])\n",
    "        self.all_data = data['images']\n",
    "        self.all_data = self._normalise_data(\n",
    "            self.all_data, lower_bound, upper_bound\n",
    "            )\n",
    "        self.all_masks = (data['masks'].sum(axis=(0,1)) > 20)\n",
    "        self.data_transform = data_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.all_data.shape[-1]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.all_data[...,idx], self.all_masks[...,idx]\n",
    "\n",
    "    def _normalise_data(self, data, lower_bound, upper_bound):\n",
    "      data = np.clip(data, lower_bound, upper_bound)\n",
    "      data = (data - lower_bound)/(upper_bound-lower_bound)\n",
    "      return data\n",
    "\n",
    "    def collate(self, batch):\n",
    "      x = np.stack([elem[0] for elem in batch])\n",
    "      y = np.stack([elem[1] for elem in batch]).astype('float32')\n",
    "      x = self.data_transform(torch.from_numpy(x).unsqueeze(1))\n",
    "      y = torch.from_numpy(y)\n",
    "      return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Vr-KYJCkryT"
   },
   "source": [
    "# Initialise Dataset and DataLoader classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toKg-zzLObff"
   },
   "outputs": [],
   "source": [
    "train_dataset = SegmentationDatasetNumpy('train', lower_bound, upper_bound)\n",
    "val_dataset = SegmentationDatasetNumpy('val', lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7tFxNrtPD0O"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    collate_fn = train_dataset.collate\n",
    "    )\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    collate_fn = val_dataset.collate\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_StQyeSlO0e"
   },
   "source": [
    "# Define a model\n",
    "\n",
    "- You could use a model from the ```timm``` library. By default we will use ```resnet50```. You can change it to your liking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KtfHGcaeC-a"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = create_model(\n",
    "    'resnet50', \n",
    "    pretrained=False, \n",
    "    in_chans=1, \n",
    "    num_classes=1\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovxDIM24mFZL"
   },
   "source": [
    "# TASK 1.\n",
    "**(1 point)**\n",
    "\n",
    "\n",
    "Define a feature extractor:\n",
    "```feature_extractor = ...```\n",
    "\n",
    "- Feature extractor is a model that processes the input image and outputs an embedding vector. The embedding vector is not a vector of logits;\n",
    "- In most of cases after the last convolutional layer one applies some pooling layer and obtains the embeddings (it might be the case that after pooling linear representation processed by some MLP);\n",
    "- In any case if the vector of logits is the output vector of the model, the embeddings vector is extracted from the pre-last (in most of cases) layer;\n",
    "- **Hint**: Use ```model.children()``` or similar methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpIbwhqk9bug"
   },
   "outputs": [],
   "source": [
    "# feature_extractor = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVN3pKjMn1f5"
   },
   "source": [
    "# Embeddings of randomly initialized network\n",
    "**(1 point)**\n",
    "\n",
    "- Get the embeddings of the randomly intialized network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ioc8WTi04An8"
   },
   "outputs": [],
   "source": [
    "_ = model.eval()\n",
    "\n",
    "# embeddings = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUyacgeA3lsU"
   },
   "source": [
    "# Define train/val loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJGxlquexBq5"
   },
   "outputs": [],
   "source": [
    "def train(model):  \n",
    "  model = model.train()\n",
    "  train_loss = 0\n",
    "  train_dice = 0\n",
    "  for batch in tqdm(train_dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    x, y = batch\n",
    "    pred = model(x.to(device)).squeeze()\n",
    "    pred = pred.cpu()\n",
    "    loss = criterion(pred, y)\n",
    "    train_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  return train_loss\n",
    "\n",
    "def val(model):\n",
    "  model = model.eval()\n",
    "  val_loss, val_acc = 0, 0\n",
    "  for batch in tqdm(val_dataloader):\n",
    "    x, y = batch\n",
    "    with torch.no_grad():\n",
    "      pred = model(x.to(device)).squeeze()\n",
    "    pred = pred.cpu()\n",
    "    loss = criterion(pred, y.float())\n",
    "    val_loss += loss.item()\n",
    "    val_acc += ((pred>0).float() == y).sum()\n",
    "  return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hf4amlNexfNW"
   },
   "source": [
    "# Define a loss function and optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHXPTnxAxawa"
   },
   "outputs": [],
   "source": [
    "criterion = BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.parameters(), lr=3e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Esofm88g3iN7"
   },
   "source": [
    "# Train a binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2B92W8_HyOTm"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  train_loss = train(model)\n",
    "  val_loss, val_acc = val(model)\n",
    "  train_loss = train_loss/len(train_dataset)\n",
    "  val_loss = val_loss/len(val_dataset)\n",
    "  val_acc = val_acc/len(val_dataset)\n",
    "  print(f'Train loss = {train_loss:.3f} after the {epoch:d} epochs')\n",
    "  print(f'Val loss = {val_loss:.3f} after the {epoch:d} epochs')\n",
    "  print(f'Val accuracy = {val_acc:.3f} after the {epoch:d} epochs')\n",
    "  print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bqe0ai433Nq"
   },
   "source": [
    "# Compare t-SNE visualisations of the trained and randomly initialised networks\n",
    "\n",
    "**(1 point)**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Plot the t-SNE embeddings of the trained network and compare them to the visualisation of the untrained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAORX56R4Vhz"
   },
   "outputs": [],
   "source": [
    "# tsne_random = ...\n",
    "# tsne_trained = ...\n",
    "# Your visualisation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0gd6EvL5xBy"
   },
   "source": [
    "#TASK 2\n",
    "\n",
    "# Expected Calibration Error\n",
    "\n",
    "- We will divide the probabilities into bins;\n",
    "- We will store bins in a dictionary; a key will correspond to the bin's number and a value will be locations of items in a tensor. Using this value we can access needed values simply using\n",
    "```probs[inds]```\n",
    "\n",
    "---\n",
    "\n",
    "# Calculate confidences and ECE\n",
    "- **(1 point)** Write the function that calculates the confidence for each bin. Recall that confidence is the average probability of items in a bin;\n",
    "- Return a ```dict``` (similar to ```calculate_bin_accuracy```);\n",
    "- **(1 point)** Write a function that calculates ECE. ECE is the averaged absolute difference between accuracies and confidences;\n",
    "- Mind ```nan```'s!\n",
    "- ECE calculation should work correctly for any number of bins!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MP8hL357Ssw"
   },
   "outputs": [],
   "source": [
    "def bin_inds(probs, y, num_bins):\n",
    "  bins = dict()\n",
    "  thrs = np.linspace(0, 1, num_bins+1)\n",
    "  for i in range(len(thrs)-1):\n",
    "    bin_locations = np.logical_and(\n",
    "            probs >= thrs[i], probs < thrs[i+1]\n",
    "        )\n",
    "    bins[i] = bin_locations\n",
    "  return bins\n",
    "\n",
    "def calculate_bin_accuracy(pred, y, bins):\n",
    "  bin_accs = dict()\n",
    "  for key, inds in bins.items():\n",
    "    acc = metrics.accuracy_score(\n",
    "        y[inds], pred[inds]>=0.5\n",
    "    )\n",
    "    if np.isnan(acc):\n",
    "      bin_accs[key] = 0\n",
    "    else:\n",
    "      bin_accs[key] = acc\n",
    "  return bin_accs\n",
    "\n",
    "def calculate_bin_confidence(pred, bins):\n",
    "  bin_confs = dict()\n",
    "  # Your code here\n",
    "  return bin_confs\n",
    "\n",
    "def calculate_ece(bin_accs, bin_confs, bins, y):\n",
    "  ece = 0\n",
    "  # Your code here\n",
    "  return ece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2Ih1jFG_acY"
   },
   "source": [
    "# Finding temperature\n",
    "**(2 point)**\n",
    "\n",
    "We will find an optimal value of the temperature using SGD. For that we need to define a set of parameters that we will optimize.\n",
    "\n",
    "---\n",
    "\n",
    "- Define a variable $\\tau$ as a pytorch's parameter;\n",
    "- Use this parameter to initialize optimizer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TkrSzvCoYwG"
   },
   "outputs": [],
   "source": [
    "# Define tau as a parameter\n",
    "# tau = ...\n",
    "# Define an optimizer\n",
    "# optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jrpTVZGAEvu"
   },
   "source": [
    "# Train loop for the temperature scaling\n",
    "\n",
    "**(2 points)**\n",
    "\n",
    "Write a train loop that performs gradient descent and finds the optimal value of the parameter $\\tau$.\n",
    "\n",
    "---\n",
    "\n",
    "Hints:\n",
    "- Use cross-entropy loss (in this binary case you could use BCEWithLogits);\n",
    "- Do not forget that you do not need to optimize model's parameters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNN_m_2oOO3Q"
   },
   "outputs": [],
   "source": [
    "def train_ece(model):  \n",
    "  train_loss = 0\n",
    "  model = model.eval()\n",
    "  # Your code here\n",
    "  \n",
    "  return train_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpcdC-6XAzPh"
   },
   "source": [
    "# Find the optimal value of $\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rygFJKpYud4h"
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "for _ in tqdm(range(epochs)):\n",
    "  train_ece(model)\n",
    "\n",
    "print(tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NujSpnt59EbQ"
   },
   "source": [
    "# Compare ECEs\n",
    "**(1 point)**\n",
    "\n",
    "---\n",
    "\n",
    "- Calculate ECE for $\\tau = 1$ (uncalibrated);\n",
    "- What value of $\\tau$ did you obtain?\n",
    "- Calculate ECE using the value of $\\tau$ obtained after temperature scaling;\n",
    "- Compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKzusLaX9tRF"
   },
   "outputs": [],
   "source": [
    "def get_val_logits(model):\n",
    "  preds = torch.zeros(len(val_dataset))\n",
    "  for i, batch in enumerate(val_dataloader):\n",
    "    x,y = batch\n",
    "    with torch.no_grad():\n",
    "      pred = model(x.to(device)).squeeze()\n",
    "    preds[16*i:16*(i+1)] = pred.cpu()\n",
    "  return preds\n",
    "\n",
    "def get_val_probs(logits, tau):\n",
    "  logits = logits/tau\n",
    "  return torch.sigmoid(logits)\n",
    "\n",
    "num_bins = 10\n",
    "\n",
    "# Your comparison here"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
